{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7454a7-cbf2-4287-afc5-57f0b70647e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\santosh\\tf-env\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: torch in c:\\users\\santosh\\tf-env\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\santosh\\tf-env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\santosh\\tf-env\\lib\\site-packages (from torch) (2025.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\santosh\\tf-env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Santosh\\tf-env\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc147ee5-eafa-4708-8338-1e7f1aa3c5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: transformers in c:\\users\\santosh\\tf-env\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: torch in c:\\users\\santosh\\tf-env\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: vaderSentiment in c:\\users\\santosh\\tf-env\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\santosh\\tf-env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\santosh\\tf-env\\lib\\site-packages (from torch) (2025.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\santosh\\tf-env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Santosh\\tf-env\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers torch vaderSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d673a51c-8ab8-4a24-9a48-b4ebf6107b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5397e97dc12945d7910f0dc8fda2e4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santosh\\tf-env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Santosh\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef86633c7c64d6c883fcd6ebd1c24be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0197cee67b004a35b6db9e0dfecd760d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  30%|###       | 115M/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ede07f609244ebc91daed10f338622c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  30%|###       | 115M/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45eec771a7c642ef88cc1ca08c3de6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  35%|###5      | 147M/415M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949efcbe03054c1b894ac530ad05167b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  41%|####1     | 189M/457M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3800bc6704bf496bb581b5bc463e598b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  47%|####7     | 241M/509M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95584f11063543a2ac48c0f0a6df37c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  47%|####7     | 241M/509M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e4bf98ef1448639bd0050c8c6ed986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9786c69786864926838d4aceea0958f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¢ Tweet (Negative): @TheRealScarab PA system bugs are a bummer, sorry.  \n",
      "ðŸ¤– GPT Response: [OpenAI Error] \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ðŸ§  VADER Score: -0.7264\n",
      "ðŸ§  BERT Sentiment: NEGATIVE (1.0)\n",
      "\n",
      "ðŸ“¢ Tweet (Negative): oh daaamnnn! the firemen ball's on the 14th and i'll still be in Cannes  So many handsome men united 2gether, and i miss it! *out tonite*\n",
      "ðŸ¤– GPT Response: [OpenAI Error] \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ðŸ§  VADER Score: -0.7264\n",
      "ðŸ§  BERT Sentiment: NEGATIVE (1.0)\n",
      "\n",
      "ðŸ“¢ Tweet (Negative): I don't want to be cold in April, but I am \n",
      "ðŸ¤– GPT Response: [OpenAI Error] \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ðŸ§  VADER Score: -0.7264\n",
      "ðŸ§  BERT Sentiment: NEGATIVE (1.0)\n",
      "\n",
      "ðŸ“¢ Tweet (Positive): heading to PA with Cara and Tielyr for Kirra's 1st birthday party! yay babies! \n",
      "ðŸ¤– GPT Response: [OpenAI Error] \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ðŸ§  VADER Score: -0.7264\n",
      "ðŸ§  BERT Sentiment: NEGATIVE (1.0)\n",
      "\n",
      "ðŸ“¢ Tweet (Positive): sitting all alone. and want someone to come and keep me company. Well, I think i should go and watch TV. \n",
      "ðŸ¤– GPT Response: [OpenAI Error] \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ðŸ§  VADER Score: -0.7264\n",
      "ðŸ§  BERT Sentiment: NEGATIVE (1.0)\n",
      "\n",
      "ðŸ“¢ Tweet (Positive): @yeqing_one dont stress! jiayou qingqing! \n",
      "ðŸ¤– GPT Response: [OpenAI Error] \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "ðŸ§  VADER Score: -0.7264\n",
      "ðŸ§  BERT Sentiment: NEGATIVE (1.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "# Force transformers to use PyTorch, not TensorFlow\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "# Set your OpenAI API Key if available\n",
    "openai.api_key = \"your-openai-api-key\"  # Replace with your actual key or keep as placeholder\n",
    "\n",
    "# Load dataset (adjust path if needed)\n",
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='latin-1', header=None)\n",
    "df.columns = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "df = df[[\"sentiment\", \"text\"]]\n",
    "df = df[df['sentiment'].isin([0, 4])]\n",
    "\n",
    "# Sample 3 negative and 3 positive tweets\n",
    "sampled = pd.concat([\n",
    "    df[df[\"sentiment\"] == 0].sample(3, random_state=1),\n",
    "    df[df[\"sentiment\"] == 4].sample(3, random_state=2)\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Initialize sentiment analyzers\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Explicitly specify the BERT model to avoid pipeline inference error\n",
    "bert_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "# Prompt template\n",
    "prompt_template = \"Respond to this tweet in an empathetic tone: \"\n",
    "\n",
    "# Function to get GPT-4 response (optional)\n",
    "def generate_response(prompt):\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return completion['choices'][0]['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"[OpenAI Error] {e}\"\n",
    "\n",
    "# Run analysis\n",
    "for i, row in sampled.iterrows():\n",
    "    tweet = row[\"text\"]\n",
    "    sentiment_label = \"Negative\" if row['sentiment'] == 0 else \"Positive\"\n",
    "    prompt = prompt_template + tweet\n",
    "\n",
    "    print(f\"\\nðŸ“¢ Tweet ({sentiment_label}): {tweet}\")\n",
    "\n",
    "    # GPT response\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"ðŸ¤– GPT Response: {response}\")\n",
    "\n",
    "    # VADER sentiment\n",
    "    vader_score = vader.polarity_scores(response)['compound']\n",
    "\n",
    "    # BERT sentiment\n",
    "    bert_result = bert_analyzer(response)[0]\n",
    "    bert_label = bert_result['label']\n",
    "    bert_conf = round(bert_result['score'], 2)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"ðŸ§  VADER Score: {vader_score}\")\n",
    "    print(f\"ðŸ§  BERT Sentiment: {bert_label} ({bert_conf})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9895e15e-7d71-49df-ae9f-be5eefd9a21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\santosh\\tf-env\\lib\\site-packages (0.32.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\santosh\\tf-env\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from huggingface_hub[hf_xet]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\santosh\\tf-env\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.13.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub[hf_xet])\n",
      "  Downloading hf_xet-1.1.2-cp37-abi3-win_amd64.whl.metadata (883 bytes)\n",
      "Requirement already satisfied: colorama in c:\\users\\santosh\\tf-env\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\santosh\\tf-env\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\n",
      "Downloading hf_xet-1.1.2-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.3/2.7 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.3/2.7 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.3/2.7 MB 13.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.6/2.7 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: hf-xet\n",
      "Successfully installed hf-xet-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\Santosh\\tf-env\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub[hf_xet]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91949502-1f0c-4724-806c-f1d00974c34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d371d6d95c949289ef419eeabe116a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santosh\\tf-env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Santosh\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-roberta-base-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f63b5b5c3a649008960c0e68e5990ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/pytorch_model.bin: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6af9c06be31401baf9c1d98bf17d1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  23%|##2       | 147M/645M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/pytorch_model.bin: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed39a45dd9c4d98979e6ed5df9d72cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  29%|##8       | 199M/698M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/pytorch_model.bin: The operation did not complete (read) (_ssl.c:2536)\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95691eb7cdf5419c8faceba81cb264be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  39%|###9      | 325M/824M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment/resolve/main/pytorch_model.bin: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3df7a19db94e7984e6a200c22869f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  42%|####2     | 367M/866M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e23a48b93443deac8c5d184efde743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ac9b9c7a604e8db9b82828dcb9e0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad35e33bd334b28a2299c5082ac1aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09285a5343f421e8a623815da84b8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¢ Tweet (Negative): @TheRealScarab PA system bugs are a bummer, sorry.  \n",
      "ðŸ¤– GPT Response: [OpenAI Error] Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope*******-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "ðŸ§  VADER Score: -0.7964\n",
      "ðŸ§  BERT Sentiment: LABEL_0 (0.52)\n",
      "\n",
      "ðŸ“¢ Tweet (Negative): oh daaamnnn! the firemen ball's on the 14th and i'll still be in Cannes  So many handsome men united 2gether, and i miss it! *out tonite*\n",
      "ðŸ¤– GPT Response: [OpenAI Error] Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope*******-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "ðŸ§  VADER Score: -0.7964\n",
      "ðŸ§  BERT Sentiment: LABEL_0 (0.52)\n",
      "\n",
      "ðŸ“¢ Tweet (Negative): I don't want to be cold in April, but I am \n",
      "ðŸ¤– GPT Response: [OpenAI Error] Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope*******-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "ðŸ§  VADER Score: -0.7964\n",
      "ðŸ§  BERT Sentiment: LABEL_0 (0.52)\n",
      "\n",
      "ðŸ“¢ Tweet (Positive): heading to PA with Cara and Tielyr for Kirra's 1st birthday party! yay babies! \n",
      "ðŸ¤– GPT Response: [OpenAI Error] Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope*******-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "ðŸ§  VADER Score: -0.7964\n",
      "ðŸ§  BERT Sentiment: LABEL_0 (0.52)\n",
      "\n",
      "ðŸ“¢ Tweet (Positive): sitting all alone. and want someone to come and keep me company. Well, I think i should go and watch TV. \n",
      "ðŸ¤– GPT Response: [OpenAI Error] Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope*******-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "ðŸ§  VADER Score: -0.7964\n",
      "ðŸ§  BERT Sentiment: LABEL_0 (0.52)\n",
      "\n",
      "ðŸ“¢ Tweet (Positive): @yeqing_one dont stress! jiayou qingqing! \n",
      "ðŸ¤– GPT Response: [OpenAI Error] Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope*******-key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "ðŸ§  VADER Score: -0.7964\n",
      "ðŸ§  BERT Sentiment: LABEL_0 (0.52)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "# For OpenAI new SDK usage\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set OpenAI API Key (replace with your actual key or set as env var)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "client = OpenAI()\n",
    "\n",
    "# Load dataset (adjust path)\n",
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='latin-1', header=None)\n",
    "df.columns = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "df = df[df['sentiment'].isin([0, 4])]\n",
    "\n",
    "# Sample 3 negative and 3 positive tweets\n",
    "sampled = pd.concat([\n",
    "    df[df[\"sentiment\"] == 0].sample(3, random_state=1),\n",
    "    df[df[\"sentiment\"] == 4].sample(3, random_state=2)\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "# Initialize sentiment analyzers\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Use a smaller, faster model for sentiment analysis from Hugging Face\n",
    "bert_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment\",\n",
    "    framework=\"pt\",\n",
    "    device=-1  # Use CPU; change to 0 if GPU available\n",
    ")\n",
    "\n",
    "prompt_template = \"Respond to this tweet in an empathetic tone: \"\n",
    "\n",
    "def generate_response(prompt):\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # or \"gpt-4\" if you have access\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return completion.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        return f\"[OpenAI Error] {e}\"\n",
    "\n",
    "for i, row in sampled.iterrows():\n",
    "    tweet = row[\"text\"]\n",
    "    prompt = prompt_template + tweet\n",
    "    print(f\"\\nðŸ“¢ Tweet ({'Negative' if row['sentiment'] == 0 else 'Positive'}): {tweet}\")\n",
    "\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"ðŸ¤– GPT Response: {response}\")\n",
    "\n",
    "    vader_score = vader.polarity_scores(response)['compound']\n",
    "\n",
    "    bert_result = bert_analyzer(response)[0]\n",
    "    bert_label = bert_result['label']\n",
    "    bert_conf = round(bert_result['score'], 2)\n",
    "\n",
    "    print(f\"ðŸ§  VADER Score: {vader_score}\")\n",
    "    print(f\"ðŸ§  BERT Sentiment: {bert_label} ({bert_conf})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4307e9e-8ab5-4566-89f4-dde0695d66db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (TensorFlow)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
